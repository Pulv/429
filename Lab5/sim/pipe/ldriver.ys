#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#Pablo Velasco
#pcv256
# nopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
#
#
# Describe how and why you modified the baseline code.
#First, I added the function iaddq in order to skip the two step irmovq -> addq in order to add numbers. The iaddq also set off flags, so it was no longer necessary to use andqs to see if numbers were > < = to 0. 
#Next, I unrolled by a factor of 8. I checked if the length - 8 > 0, then it would loop through the 8-unrolled code. Once the length was < 8, it would check if it was even. If it was not, it would go through a single loop, which would make the length even. Then it would roll through the even loop until it finished.
#I moved some of the commands so that there would be no data dependencies (mr -> mr -> ... rm -> rm -> ...). This included placing blocks of code so it would fall into other blocks rather than take jumps (Even -> Done since it will end on even most of the time). 
#I also changed some values to hex in order to read the value quicker (it was a .02 increase)
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:
##################################################################
# You can modify this portion
        # Loop header
        #r11 for even
        xorq %rax,%rax         # count = 0;
        andq %rdx,%rdx          # len <= 0?
        jle Done                # if so, goto Done:
	iaddq $-8, %rdx
	jle test
Eight:
	mrmovq (%rdi), %rcx
	mrmovq 0x8(%rdi), %rbx
	mrmovq 0x10(%rdi), %r14
	mrmovq 0x18(%rdi), %r8
	mrmovq 0x20(%rdi), %r9
	mrmovq 0x28(%rdi), %r10
	mrmovq 0x30(%rdi), %r11
	mrmovq 0x38(%rdi), %r12
	rmmovq %rcx, (%rsi)
	rmmovq %rbx, 0x8(%rsi)
	rmmovq %r14, 0x10(%rsi)
	rmmovq %r8, 0x18(%rsi)
	rmmovq %r9, 0x20(%rsi)
	rmmovq %r10, 0x28(%rsi)
	rmmovq %r11, 0x30(%rsi)
	rmmovq %r12, 0x38(%rsi)
	andq %rcx, %rcx
	jle first
	iaddq $0x1, %rax
first:
	andq %rbx, %rbx
	jle second
	iaddq $0x1, %rax
second:
	andq %r14, %r14
	jle third
	iaddq $0x1, %rax
third:
	andq %r8, %r8
	jle fourth
	iaddq $0x1, %rax
fourth:
	andq %r9, %r9
	jle fifth
	iaddq $0x1, %rax
fifth:
	andq %r10, %r10
	jle sixth
	iaddq $0x1, %rax
sixth:
	andq %r11, %r11
	jle seventh
	iaddq $0x1, %rax
seventh:
	andq %r12, %r12
	jle done
	iaddq $0x1, %rax
done:
	iaddq $0x40, %rdi
	iaddq $0x40, %rsi
	iaddq $-8, %rdx
	jg Eight
test:
	iaddq $0x8, %rdx
	irmovq $0x1, %r13
	andq %rdx, %r13
	je Even
Loop:
        mrmovq (%rdi), %r10   
	rmmovq %r10, (%rsi)
	andq %r10, %r10
        jle Npos          
        iaddq $0x1, %rax
Npos:
        iaddq $0x8, %rdi      
        iaddq $0x8, %rsi       
        iaddq $-1, %rdx    
	je Done
Even:
        mrmovq (%rdi), %r12
        mrmovq 0x8(%rdi), %r11
        rmmovq %r12, (%rsi)
        rmmovq %r11, 0x8(%rsi)
        andq %r12, %r12
        jle notPos
        iaddq $0x1, %rax
notPos:
        andq %r11, %r11
        jle finishEven
        iaddq $0x1, %rax
finishEven:
        iaddq $0x10, %rdi
        iaddq $0x10, %rsi
        iaddq $-2, %rdx
        jg Even
##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
        ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad 2
	.quad 3
	.quad -4
	.quad 5
	.quad -6
	.quad 7
	.quad 8
	.quad 9
	.quad -10
	.quad 11
	.quad 12
	.quad 13
	.quad -14
	.quad 15
	.quad -16
	.quad 17
	.quad -18
	.quad 19
	.quad -20
	.quad 21
	.quad -22
	.quad -23
	.quad -24
	.quad 25
	.quad -26
	.quad -27
	.quad 28
	.quad -29
	.quad 30
	.quad -31
	.quad -32
	.quad 33
	.quad -34
	.quad 35
	.quad -36
	.quad -37
	.quad 38
	.quad 39
	.quad 40
	.quad -41
	.quad -42
	.quad -43
	.quad -44
	.quad 45
	.quad -46
	.quad 47
	.quad -48
	.quad -49
	.quad -50
	.quad -51
	.quad -52
	.quad -53
	.quad -54
	.quad 55
	.quad 56
	.quad 57
	.quad 58
	.quad 59
	.quad -60
	.quad 61
	.quad 62
	.quad 63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
